{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code and pipeline written by **Matt Mortimer** <br />\n",
    "Contributions by Matt Spence (see specific scripts)<br />\n",
    "started 26 Oct 2021<br />\n",
    "matthew.mortimer@anu.edu.au<br />\n",
    "Orcid ID: https://orcid.org/0000-0002-8135-9319<br />\n",
    "Python 3.8<br />\n",
    "\n",
    "**IMPORTANT: Biopython 1.79 only runs on python 3.6, 3.7 or 3.8**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview - GPAT project\n",
    "**Background:** <br/>\n",
    "[Proivde background on the enzyme of interest including the purpose of the SSN]<br/>\n",
    "Seed sequence length (aa): [insert sequence length]<br/>\n",
    "[Insert details of protein family and sequence similarity information]<br/>\n",
    "**Aim:** <br/>\n",
    "[Insert project aim] <br/> \n",
    "**Data sources:**  <br/>\n",
    "PFAM ([insert pfam id for protein family])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dependencies:**<br /> **[Make sure you install these dependencies]** <br/>\n",
    "1.    Biopython\n",
    "2.    BioServices\n",
    "3.    Pandas\n",
    "4.    Seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Custom module versions**<br />\n",
    "[Modules used in this template] <br/>\n",
    "1. analysis v2.1.0\n",
    "2. annotations v1.3.2\n",
    "3. cleaner v1.3.3\n",
    "4. log v1.2.0\n",
    "5. progress v1.0.1 \n",
    "6. run_blast v1.1.2\n",
    "7. size_filter v1.0.3\n",
    "8. uniprot v1.0.2\n",
    "9. utilities v1.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Custom scripts\n",
    "from modules.annotations import *\n",
    "from modules.cleaner import *\n",
    "from modules.analysis import len_distro\n",
    "from modules.run_blast import blast\n",
    "from modules.size_filter import *\n",
    "from modules.uniprot import uniprot\n",
    "from modules.utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set constant variables for this notebook\n",
    "DATE = datetime.now().strftime('%y%m%d')\n",
    "PROJECT = '[insert project name here with no spaces]'\n",
    "PFAM_ID = '[insert PFAM id here]'\n",
    "\n",
    "# Check working dir, seperately make sure the working dir has \n",
    "# two subdirs one called input and another output. Have a sub dir\n",
    "# in output called BLAST (all capitals)\n",
    "import pathlib\n",
    "print(pathlib.Path().resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve PFAM family as fasta file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The uniprot function takes 3 arguments, the pfam id, and name\n",
    "# for the full family downloaded fasta file, and then the file name \n",
    "# of the filtered dataset containing only sequences from swissprot \n",
    "# (reviewed)\n",
    "output = f\"{PROJECT}input/{DATE}_pfam_{PFAM_ID}_all.fasta\"\n",
    "sp_output = f\"output/{DATE}_pfam_{PFAM_ID}_swissprot.fasta\"\n",
    "\n",
    "uniprot(PFAM_ID, output, sp_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence assessment and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the outputted file from above and pass to len_distro which will provide a \n",
    "# seaborn displot, can add bin width and, x and y size as arguments if necessary. \n",
    "# This function is used to assess size distribution of sequences which can be \n",
    "# filtered if necessary\n",
    "\n",
    "# Here I've used a binwidth of 10\n",
    "\n",
    "len_distro(f'{PROJECT}/input/{DATE}_pfam_{PFAM_ID}_all.fasta', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes a fasta file, project name, data source as arguments. \n",
    "# Stipulate 'greater' or 'less' and the length in resiudes as arguments to \n",
    "# filter out others that do not meet that condition. \n",
    "\n",
    "# Here I've retained sequences of < than 800 residues in length, discarding \n",
    "# the rest.   \n",
    "\n",
    "size_filter('{PROJECT}/input/{DATE}_pfam_{PFAM_ID}_all.fasta', PROJECT, 'PFAM', 'less', 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the outputted file from above and pass to len_distro which will provide a \n",
    "# seaborn displot, can add bin width and, x and y size as arguments if necessary. \n",
    "# This function is used to assess size distrobution of sequences which can be \n",
    "# filtered if required\n",
    "\n",
    "# Here I've used a binwidth of 10\n",
    "\n",
    "len_distro('[insert the file name from the output above]', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the distribution above I've taken the output from above and retained \n",
    "# sequences < than 360 residues in length, discarding the rest.   \n",
    "\n",
    "size_filter('[insert the file name from the output above]', PROJECT, 'PFAM', 'less', 360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the distribution above I've taken the output from above and retained \n",
    "# sequences > than 240 residues in length, discarding the rest.   \n",
    "\n",
    "size_filter('[insert the file name from the output above]', PROJECT, 'PFAM', 'greater', 240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check the filtering by passing the output from 'size_filter' \n",
    "# function back to the len_distro function. \n",
    "\n",
    "len_distro('[insert the file name from the output above]', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the distribution above I've taken the output from above and retained \n",
    "# sequences < than 320 residues in length, discarding the rest.   \n",
    "\n",
    "size_filter('[insert the file name from the output above]', PROJECT, 'PFAM', 'less', 320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the distribution above I've taken the output from above and retained \n",
    "# sequences > than 280 residues in length, discarding the rest.   \n",
    "\n",
    "size_filter('[insert the file name from the output above]', PROJECT, 'PFAM', 'greater', 280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function pulls back the UniProt annotations for the sequences \n",
    "# filtered by size. Takes 3 arguments, the input file, the data source\n",
    "# and the project name. Generates a text file in tab seperated csv format\n",
    "\n",
    "# This function takes a very long time to run, each sequence must be \n",
    "# individually passed back to the served and then wait for the reply, \n",
    "# sequentially. If the uniprot server is busy it can take longer still.\n",
    "\n",
    "input_f = '[insert the file name from the output above]'\n",
    "\n",
    "annotations(input_f, 'PFAM', PROJECT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This indexing function takes the annotation file generated above and \n",
    "# the last generated fasta file then adds the sequences to the annotations\n",
    "#  generating a 'master index' csv file. This can act a reference going \n",
    "# forward and a repository of un-edited sequences.\n",
    "\n",
    "# The function takes 3 arguments, the annotation text file generated from \n",
    "# the annotations function, a related fasta file, and the project name. \n",
    "\n",
    "# The function works by merging pandas dataframes as 'inner' on the 'Entry'\n",
    "# name, so if a sequence Entry name is missing from the annotation file\n",
    "# that sequence is lost.\n",
    "\n",
    "annotation_file = '[insert name of the annotation .txt file here]'\n",
    "sequence_file = '[insert name of the file that was input for the annotations function above]'\n",
    "\n",
    "indexing(annotation_file, sequence_file, PROJECT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check data in excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to filter out the duplicate sequences, however if one of the\n",
    "# copies has Status 'reviewed' we need to keep that copy not the \n",
    "# unreviewed one. To do this we need to split the dataset into\n",
    "# reviewed and unreviewed sequences, remove duplicates from within\n",
    "# those subsets before remove duplicates between the subsets\n",
    "\n",
    "reviewed('[insert name of mast_index.csv file from above]', 'GPAT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes the fasta files generated above and passes them seperately to the\n",
    "# sequence_cleaner function, with datasource and project name as arguments. \n",
    "# Removes duplicate sequences, and sequences with ambigious residues. \n",
    "# There maybe duplications between the two files however \n",
    "\n",
    "cleaner(f'{PROJECT}/output/{DATE}_reviewed.fasta', 'PFAM_reviewed', PROJECT)\n",
    "\n",
    "print('') # Line spacer, just makes the output easier to read\n",
    "\n",
    "cleaner(f'{PROJECT}/output/{DATE}_unreviewed.fasta', 'PFAM_unreviewed', PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewed_file = f'{PROJECT}/output/{DATE}_reviewed.fasta'\n",
    "unreviewed_file = f'{PROJECT}/output/{DATE}_unreviewed.fasta'\n",
    "\n",
    "reviewed_unreviewed(reviewed_file, unreviewed_file, 'PFAM', PROJECT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Network file using blast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a all v all BLAST search using a blast function \n",
    "# BLAST must be installed locally.\n",
    "# Takes 3 arguments, a fasta file, the project name, and the e-value threshold.\n",
    "\n",
    "# Just a clear way of laying out the function arguments\n",
    "\n",
    "in_fasta = 'output/211119_DGAT_PFAM_rev_unrev_deduped.fasta' # Use the last outputted \n",
    "                                                        # fasta file from above\n",
    "# path = '/usr/lib/ncbi-blast-2.12.0+/bin'\n",
    "\n",
    "# Actual BLAST function, using default E_value_threshold=\"10e-10\", cpus=\"2\" (adjust \n",
    "# these arguments as required)\n",
    "\n",
    "blast(in_fasta, PROJECT, '10e-100', cpus='15')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format network file for SSN\n",
    "The section below uses Pandas to add taxonomic information from the master index created above to the network file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set input and output files as variables \n",
    "input_network = f'{PROJECT}/output/BLAST/{DATE}_dataset_network_10e-10.csv'\n",
    "input_index = f'{PROJECT}/output/{DATE}_PFAM_master_index.csv'\n",
    "output_network = f'{PROJECT}/output/BLAST/{DATE}_SSN_10e-10.csv'\n",
    "\n",
    "# Read the input files\n",
    "netwrk = pd.read_csv(input_network)\n",
    "master_index = pd.read_csv(input_index)\n",
    "\n",
    "# Make a new dataframe for the input_files with a subset of columns, then rename\n",
    "# the first column of the input_network dataframe so that it can be easily merged \n",
    "# with the index file\n",
    "nw_mod = netwrk[['Query', 'Target', '% Identity', 'Length', 'E-value', 'Bit-score']]\n",
    "nw_mod.columns = ['Entry', 'Target', '% Identity', 'Length', 'E-value', 'Bit-score']\n",
    "\n",
    "index = master_index[['Entry', 'Status', 'Protein names', 'Gene names', 'Organism']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dataframe\n",
    "nw_mod.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dataframe\n",
    "index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes on the Entry column. This will add the \n",
    "# taxonomic information to the network file\n",
    "merged = nw_mod.merge(index, on='Entry', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dataframe, didn't use .head() as need to see the \n",
    "# last rows.\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the new network dataframe to file\n",
    "merged.to_csv(output_network, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "626bbb74384245b4d158eff61d8a861d9ebdc0ed211d7e5a2090e25f0e40b3d0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
